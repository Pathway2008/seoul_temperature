{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "1qiSrcH7NcrE",
        "c6ti2FZeNigp",
        "uHP-cv7F49li"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 라이브러리 호출"
      ],
      "metadata": {
        "id": "EPmZ7_UR0zPf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "URxY_HjdNUy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c7f344-ef85-49a9-c8d0-b6edd59469ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n",
            "Collecting optuna\n",
            "  Downloading optuna-3.5.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.4/413.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.23)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.0 alembic-1.13.1 colorlog-6.8.0 optuna-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import os\n",
        "import pickle\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import weight_norm\n",
        "import math\n",
        "import sys\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import copy\n",
        "import torchvision.ops"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 깃 클론"
      ],
      "metadata": {
        "id": "gbQoomyU01uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone 'https://github.com/edwardhan925192/TimesNet.git'\n",
        "%cd /content/TimesNet\n",
        "\n",
        "from times_model import Model\n",
        "from whole_dataset import TimeSeriesDataset,TimeSeries_ValDataset,TimeSeries_TestDataset\n",
        "from schedular.scheduler import initialize_scheduler"
      ],
      "metadata": {
        "id": "s8DPLm4z_5_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31b1713-928c-403e-e66e-35667a98ae23"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TimesNet'...\n",
            "remote: Enumerating objects: 844, done.\u001b[K\n",
            "remote: Counting objects: 100% (385/385), done.\u001b[K\n",
            "remote: Compressing objects: 100% (225/225), done.\u001b[K\n",
            "remote: Total 844 (delta 221), reused 270 (delta 156), pack-reused 459\u001b[K\n",
            "Receiving objects: 100% (844/844), 281.87 KiB | 1.61 MiB/s, done.\n",
            "Resolving deltas: 100% (482/482), done.\n",
            "/content/TimesNet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시드 설정"
      ],
      "metadata": {
        "id": "xPo1jFuC2gmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 0\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)  # for multi-GPU.\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)  # Replace 42 with your chosen seed number"
      ],
      "metadata": {
        "id": "hqGr2CfmullZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qiSrcH7NcrE"
      },
      "source": [
        "# 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y67i-ILBNdNq"
      },
      "outputs": [],
      "source": [
        "from pandas.tseries.offsets import YearEnd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# validation 스플릿\n",
        "def split_train_validation_timeseries(df, validation_ranges):\n",
        "    '''\n",
        "    takes list of validation range,\n",
        "    Ex)   [(800, 900), (900, 1000)]\n",
        "\n",
        "    returns rows previous of validation range and validation range.\n",
        "    '''\n",
        "    train_dfs = []\n",
        "    validation_dfs = []\n",
        "\n",
        "    for val_range in validation_ranges:\n",
        "        # Ensure the range is valid\n",
        "        start, end = val_range\n",
        "        if start >= end or end > len(df):\n",
        "            raise ValueError(\"Invalid range: {}\".format(val_range))\n",
        "\n",
        "        # Split the DataFrame\n",
        "        validation_df = df.iloc[start:end]\n",
        "        train_df = df.iloc[:start]\n",
        "\n",
        "        train_dfs.append(train_df)\n",
        "        validation_dfs.append(validation_df)\n",
        "\n",
        "    return train_dfs, validation_dfs\n",
        "\n",
        "# 결측치 채우기\n",
        "def fill_missing_with_ratio(df, ratio, column2, column1):\n",
        "    \"\"\"\n",
        "    If column 1 is missing, fill it by multiplying column 2 with the ratio.\n",
        "    If column 2 is missing, fill it by dividing column 1 by the ratio.\n",
        "\n",
        "    column 2 is bigger column\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame to process.\n",
        "    ratio (float): The ratio to be used for filling missing values.\n",
        "    column1 (str): The name of the first column.\n",
        "    column2 (str): The name of the second column.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with filled values.\n",
        "    \"\"\"\n",
        "    if column1 not in df or column2 not in df:\n",
        "        raise ValueError(\"Specified columns must be in the DataFrame\")\n",
        "\n",
        "    # If column1 is missing, fill it by multiplying column2 with the ratio\n",
        "    df[column1] = df.apply(lambda row: row[column2] * ratio if pd.isna(row[column1]) else row[column1], axis=1)\n",
        "\n",
        "    # If column2 is missing, fill it by dividing column1 by the ratio\n",
        "    df[column2] = df.apply(lambda row: row[column1] / ratio if pd.isna(row[column2]) else row[column2], axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "# 결측치 채우기2\n",
        "def interpolate_columns(df, columns):\n",
        "    \"\"\"\n",
        "    Interpolates missing values in specified columns of a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df (pd.DataFrame): The DataFrame to process.\n",
        "    columns (list of str): List of column names in which to interpolate missing values.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with interpolated values in the specified columns.\n",
        "    \"\"\"\n",
        "    for column in columns:\n",
        "        if column in df.columns:\n",
        "            df[column] = df[column].interpolate(method='linear', limit_direction='forward', axis=0)\n",
        "        else:\n",
        "            print(f\"Column '{column}' not found in DataFrame.\")\n",
        "    return df\n",
        "\n",
        "def convert_datetime_column(df, column_name):\n",
        "    # Define a helper function to extract date-time components and differentiate between weekends and weekdays\n",
        "    def extract_date_time_components(datetime_str):\n",
        "        # Check if the hour component is present in the datetime string\n",
        "        if ':' in datetime_str or ' ' in datetime_str and len(datetime_str.split(' ')[1]) > 0:\n",
        "            dt_format = '%Y-%m-%d %H'\n",
        "        else:\n",
        "            dt_format = '%Y-%m-%d'\n",
        "\n",
        "        dt_obj = datetime.strptime(datetime_str, dt_format)\n",
        "        weekday = dt_obj.weekday()\n",
        "\n",
        "        # Classify as 'Weekend' or 'Weekday'\n",
        "        day_type = 'Weekend' if weekday in [5, 6] else 'Weekday'\n",
        "\n",
        "        # Return components based on the presence of the hour component\n",
        "        if dt_format == '%Y-%m-%d %H':\n",
        "            return dt_obj.year, dt_obj.month, dt_obj.day, dt_obj.hour, day_type\n",
        "        else:\n",
        "            return dt_obj.year, dt_obj.month, dt_obj.day, None, day_type\n",
        "\n",
        "    # Apply the helper function to the specified column\n",
        "    components = df[column_name].apply(extract_date_time_components)\n",
        "\n",
        "    # Unpack components and create columns conditionally\n",
        "    df['Year'], df['Month'], df['Day'], hour_component, df['DayType'] = zip(*components)\n",
        "    if any(hour is not None for hour in hour_component):\n",
        "        df['Hour'] = hour_component\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "columns_info = {\n",
        "    'Month': 12,\n",
        "    'Hour': 23,\n",
        "    'Day':31\n",
        "}\n",
        "\n",
        "def fill_column_from_dfB_to_dfA(dfA, dfB, column_name):\n",
        "    \"\"\"\n",
        "    Fill values of a column in dfA with values from the same column in dfB.\n",
        "\n",
        "    Parameters:\n",
        "    dfA (pd.DataFrame): The dataframe to be updated.\n",
        "    dfB (pd.DataFrame): The dataframe providing the new values.\n",
        "    column_name (str): The name of the column to be updated.\n",
        "    \"\"\"\n",
        "    # Check if the column exists in both dataframes\n",
        "    if column_name in dfA.columns and column_name in dfB.columns:\n",
        "        # Determine the number of rows to update\n",
        "        num_rows_to_update = min(len(dfA), len(dfB))\n",
        "\n",
        "        # Update the values in dfA from dfB\n",
        "        dfA.loc[:num_rows_to_update - 1, column_name] = dfB.loc[:num_rows_to_update - 1, column_name]\n",
        "    else:\n",
        "        raise ValueError(\"Column not found in one or both dataframes\")\n",
        "\n",
        "    return dfA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUcBo-EYNdh5"
      },
      "source": [
        "# 데이터 결측치 처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WIWbDLvENeR6"
      },
      "outputs": [],
      "source": [
        "from validation_split import split_train_validation_timeseries\n",
        "train = pd.read_csv('train path')\n",
        "sample_submission = pd.read_csv('sample submission path')\n",
        "\n",
        "train = convert_datetime_column(train, '일시')\n",
        "test = convert_datetime_column(sample_submission.drop('평균기온',axis = 1), '일시')\n",
        "train = train.drop(['일시','DayType'],axis = 1)\n",
        "test = test.drop(['일시','DayType'],axis = 1)\n",
        "\n",
        "# 최고기온, 최저기온, 일교차 , 평균풍속 결측치\n",
        "train_ = interpolate_columns(train,['최고기온','최저기온', '일교차' , '평균풍속'])\n",
        "\n",
        "# 강수량 결측치\n",
        "train_['강수량'] = train_['강수량'].fillna(0)\n",
        "\n",
        "# 일조합 일조율 결측치\n",
        "ratio = train[train['일조율'] > 0]['일조율'].sum() / train[train['일조율'] > 0]['일조합'].sum()\n",
        "train_ = fill_missing_with_ratio(train_,ratio,'일조합','일조율')\n",
        "\n",
        "# 일사합 결측치\n",
        "train_ = train_.drop('일사합',axis =1 )\n",
        "train_ = train_.drop(['평균풍속','일교차','평균풍속','일조율','Year','Month','Day'],axis = 1)\n",
        "train_ = train_[['평균기온','최고기온','최저기온']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6ti2FZeNigp"
      },
      "source": [
        "# 겨울 모델 및 스케줄러 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HG75LH0aNiQ5"
      },
      "outputs": [],
      "source": [
        "# 스케줄러 설정\n",
        "class SchedulerConfig:\n",
        "    def __init__(self):\n",
        "        # STARTS from lr goes down to eta_min in T_0\n",
        "        self.CosineAnnealingWarmRestarts = {'T_0': 10, 'T_mult': 1, 'eta_min': 0.0005}\n",
        "        self.StepLR = {'step_size': 10, 'gamma': 0.1}\n",
        "        self.ExponentialLR = {'gamma': 0.95}\n",
        "\n",
        "        #steps_per_epoch should be set to number of batches , epochs should be total number of epochs\n",
        "        # Starts from low lr to max lr\n",
        "        self.OneCycleLR = {'max_lr': 0.01, 'steps_per_epoch': 10, 'epochs': 20}\n",
        "\n",
        "        self.CyclicLR = {'base_lr': 0.001, 'max_lr': 0.01, 'step_size_up': 5,'step_size_down':5,  'mode': 'triangular'}\n",
        "\n",
        "    def get_params(self, scheduler_name):\n",
        "        return getattr(self, scheduler_name, None)\n",
        "\n",
        "num_features = 3\n",
        "target_name = '평균기온'\n",
        "\n",
        "# 모델 파라미터 설정\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.task_name = 'short_term_forecast'\n",
        "        self.seq_len = 365\n",
        "        self.window_shift = 1\n",
        "        self.enc_in = num_features\n",
        "        self.d_model = 22\n",
        "        self.top_k = 3\n",
        "        self.d_ff = 22\n",
        "        self.num_kernels = 6\n",
        "        self.dropout = 0.24915726313968972\n",
        "        self.e_layers = 1\n",
        "        self.label_len = num_features\n",
        "        self.target_col = target_name\n",
        "        self.cnn_type = 'inceptionv1'\n",
        "        self.pred_len = 358\n",
        "        self.c_out = 1\n",
        "        self.eval_range = 0\n",
        "        self.seq_range = np.concatenate([np.arange(0, 45), np.arange(317, 358)])\n",
        "        self.scheduler_config = SchedulerConfig()\n",
        "        self.scheduler_name = 'CosineAnnealingWarmRestarts'\n",
        "        self.scheduler_update_type = 'epoch'\n",
        "    def update(self, new_params):\n",
        "        for key, value in new_params.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "winter_configs = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 봄,여름,가을 모델 및 스케줄러 파라미터 설정"
      ],
      "metadata": {
        "id": "uHP-cv7F49li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SchedulerConfig:\n",
        "    def __init__(self):\n",
        "        # STARTS from lr goes down to eta_min in T_0\n",
        "        self.CosineAnnealingWarmRestarts = {'T_0': 10, 'T_mult': 1, 'eta_min': 0.0005}\n",
        "        self.StepLR = {'step_size': 10, 'gamma': 0.1}\n",
        "        self.ExponentialLR = {'gamma': 0.95}\n",
        "\n",
        "        #steps_per_epoch should be set to number of batches , epochs should be total number of epochs\n",
        "        # Starts from low lr to max lr\n",
        "        self.OneCycleLR = {'max_lr': 0.01, 'steps_per_epoch': 10, 'epochs': 20}\n",
        "\n",
        "        self.CyclicLR = {'base_lr': 0.001, 'max_lr': 0.01, 'step_size_up': 5,'step_size_down':5,  'mode': 'triangular'}\n",
        "\n",
        "    def get_params(self, scheduler_name):\n",
        "        return getattr(self, scheduler_name, None)\n",
        "\n",
        "num_features = 3\n",
        "target_name = '평균기온'\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.task_name = 'short_term_forecast'\n",
        "        # Output features and c_out should be the same when the task is anomaly_detection\n",
        "        self.seq_len = 365\n",
        "        self.window_shift = 1\n",
        "        self.enc_in = num_features    # Features\n",
        "        self.d_model = 20     # Convolution Embedding dimension AFTER RESHAPING\n",
        "        self.top_k = 3        # FFT frequency\n",
        "        self.d_ff = 20       # Convolution Output layer dimension AFTER RESHAPING\n",
        "        self.num_kernels = 6  # inception block Num of different grid cells used / If using dcvn set it to 3\n",
        "        self.dropout = 0.1933493411095017    # Dropout rate\n",
        "        self.e_layers = 1     # num Timeblock\n",
        "        self.label_len = num_features   # Features\n",
        "        self.target_col = target_name   # Name of target column\n",
        "        self.cnn_type = 'inceptionv1' # dcvn (KERNEL = 3), inceptionv1, inceptionv2, res_dcvn, res_inceptionv1, res_inceptionv2\n",
        "        self.pred_len = 358   # Prediction length\n",
        "        self.c_out = 1        # Output feature\n",
        "        self.eval_range = 0\n",
        "        self.seq_range = np.arange(45, 317)\n",
        "        self.scheduler_config = SchedulerConfig()\n",
        "        self.scheduler_name = 'CosineAnnealingWarmRestarts' #'CosineAnnealingWarmRestarts', 'StepLR', 'ExponentialLR', 'OneCycleLR', 'CyclicLR'\n",
        "        self.scheduler_update_type = 'epoch' # epoch, batch\n",
        "    def update(self, new_params):\n",
        "        for key, value in new_params.items():\n",
        "            if hasattr(self, key):\n",
        "                setattr(self, key, value)\n",
        "\n",
        "xwinter_configs = Config()"
      ],
      "metadata": {
        "id": "S8nCAzjm4-qy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpkEsqKON0Oi"
      },
      "source": [
        "# 훈련 파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = train_\n",
        "target_col = None\n",
        "learning_rate = 0.01\n",
        "num_epochs = 10\n",
        "batch_sizes = 30\n",
        "configs = winter_configs #winter_configs, xwinter_configs\n",
        "df_test = train_"
      ],
      "metadata": {
        "id": "W90lrLfDufyS"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 겨울 모델 트레이닝"
      ],
      "metadata": {
        "id": "ADcjcvlX2B0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_state = None\n",
        "\n",
        "training_loss_history = []\n",
        "\n",
        "col_list = list(df_train.columns)\n",
        "target_index = col_list.index(target_col) if target_col in col_list else -1\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "model = Model(configs).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = initialize_scheduler(optimizer, configs)\n",
        "\n",
        "train_dataset = TimeSeriesDataset(df_train, configs.seq_len, configs.pred_len, configs.seq_range, configs.eval_range)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_sizes, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (batch_data, batch_target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        batch_data, batch_target = batch_data.to(device), batch_target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_data)\n",
        "\n",
        "        loss = criterion(outputs, batch_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    average_training_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training_Loss = {average_training_loss}')\n",
        "model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "test_dataset = TimeSeries_TestDataset(df_test, configs.seq_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_sizes, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_test_data in test_loader:\n",
        "        batch_test_data = batch_test_data.to(device)\n",
        "        outputs = model(batch_test_data)\n",
        "\n",
        "winter_pred = outputs"
      ],
      "metadata": {
        "id": "aH3PmHnNeE1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64134b9e-c45d-443f-b1c6-4181afeae50e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 743/743 [01:27<00:00,  8.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Training_Loss = 3.709547520646497\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 봄,여름,가을 모델 트레이닝"
      ],
      "metadata": {
        "id": "rBwUhLK32Hkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "configs = xwinter_configs\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_state = None\n",
        "\n",
        "training_loss_history = []\n",
        "\n",
        "col_list = list(df_train.columns)\n",
        "target_index = col_list.index(target_col) if target_col in col_list else -1\n",
        "\n",
        "criterion = nn.L1Loss()\n",
        "\n",
        "model = Model(configs).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = initialize_scheduler(optimizer, configs)\n",
        "\n",
        "train_dataset = TimeSeriesDataset(df_train, configs.seq_len, configs.pred_len, configs.seq_range, configs.eval_range)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_sizes, shuffle=False)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_idx, (batch_data, batch_target) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        batch_data, batch_target = batch_data.to(device), batch_target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_data)\n",
        "\n",
        "        loss = criterion(outputs, batch_target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    average_training_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Training_Loss = {average_training_loss}')\n",
        "model_state = copy.deepcopy(model.state_dict())\n",
        "\n",
        "test_dataset = TimeSeries_TestDataset(df_test, configs.seq_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_sizes, shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_test_data in test_loader:\n",
        "        batch_test_data = batch_test_data.to(device)\n",
        "        outputs = model(batch_test_data)\n",
        "\n",
        "\n",
        "xwinter_pred = outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZAlpFGU5V8s",
        "outputId": "5a10e735-c8bf-47fe-e9a3-cc0d445606ef"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1: 100%|██████████| 743/743 [01:29<00:00,  8.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1, Training_Loss = 3.324030128333963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 계절별 모델 합치기"
      ],
      "metadata": {
        "id": "w-5m9x0P4x-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first = winter_pred[0][:45].cpu()\n",
        "third = winter_pred[0][45:].cpu()\n",
        "\n",
        "second = xwinter_pred[0].cpu()\n",
        "\n",
        "full_pred = np.concatenate([first.numpy(), second.numpy(), third.numpy()], axis=0)"
      ],
      "metadata": {
        "id": "fH5BLWBs4xC9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 제출"
      ],
      "metadata": {
        "id": "xQ-trB3RKKRK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBwCkavTtBFw"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(full_pred)\n",
        "result.columns = ['평균기온']\n",
        "submission_A = fill_column_from_dfB_to_dfA(sample_submission,result,'평균기온')\n",
        "submission_A.to_csv('submission.csv',index = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Private 점수 복원"
      ],
      "metadata": {
        "id": "PgHk1UwR6M7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 웨이트 저장공간\n",
        "# https://drive.google.com/drive/folders/1uwKoVtbX5djdrf6e4pmWj4t64LzHv9BU?usp=drive_link"
      ],
      "metadata": {
        "id": "NsrJTt3L6XNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "winter_model_path = '/content/timesnet_o_winter'\n",
        "xwinter_model_path = '/content/timesnet_x_winter.pth'\n",
        "\n",
        "winter_model_dict = torch.load(winter_model_path)\n",
        "xwinter_model_dict = torch.load(xwinter_model_path)"
      ],
      "metadata": {
        "id": "zXhfiAqOAR9I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = winter_configs\n",
        "model = Model(configs).to(device)\n",
        "\n",
        "model.load_state_dict(winter_model_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# 데이터 준비\n",
        "test_dataset = TimeSeries_TestDataset(df_test, configs.seq_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# 예측\n",
        "with torch.no_grad():\n",
        "    for batch_test_data in test_loader:\n",
        "        batch_test_data = batch_test_data.to(device)\n",
        "        outputs = model(batch_test_data)\n",
        "\n",
        "        winter_predictions = outputs\n"
      ],
      "metadata": {
        "id": "viAoGhyF6N_M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configs = xwinter_configs\n",
        "model = Model(configs).to(device)\n",
        "model.load_state_dict(xwinter_model_dict)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# 데이터 준비\n",
        "test_dataset = TimeSeries_TestDataset(df_test, configs.seq_len)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# 예측\n",
        "with torch.no_grad():\n",
        "    for batch_test_data in test_loader:\n",
        "        batch_test_data = batch_test_data.to(device)\n",
        "        outputs = model(batch_test_data)\n",
        "\n",
        "        xwinter_predictions = outputs"
      ],
      "metadata": {
        "id": "1nD6eSCtAJ4c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 계절별 복원값 합치기"
      ],
      "metadata": {
        "id": "dZm4CbOo52PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first = winter_predictions[0][:45].cpu()\n",
        "third = winter_predictions[0][45:].cpu()\n",
        "\n",
        "second = xwinter_predictions[0].cpu()\n",
        "\n",
        "bokwon = np.concatenate([first.numpy(), second.numpy(), third.numpy()], axis=0)\n",
        "bokwon"
      ],
      "metadata": {
        "id": "E4q3MFrr4l5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}